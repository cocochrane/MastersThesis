{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the class SlidingWindowDataCollection which collects the necessary data and splits it for nested cross-validation.  \n",
    "\n",
    "### Class Initialization\n",
    "The class takes multiple parameters:  \n",
    "1. participant_list: the participant_list attribute of a Participants class object\n",
    "2. sleep_info: dictionary that specifies the sleep information to use  \n",
    "    - nPreviousNights: number of previous nights of sleep information to use\n",
    "    - sleepFeatures: the features from the SleepInformation.csv file to use\n",
    "3. test_info: dictionary that specifies the test information to use  \n",
    "    - nPreviousTimesteps: specifies the number of previous test scores to use\n",
    "    - previousTestFeatures: specific test/feature combinations to use as a list of lists. For example, [['PVT','MeanInverseRT'],['DSST','CORRECT']] includes the MeanInverseRT feature from the PVT test and the CORRECT feature from the DSST test.\n",
    "    - timingInfoPreviousTests: list of timing features to include for each of the previous tests. The options are HoursAwake, NumWPonFD, CircadianPhase, NumWeekOnFD, SleepOpp72, HoursOnProtocol, SESSION, or WP. \n",
    "    - timeInfoCurrentTest: list specifies the timing features to use for the test being predicted. Same options as above.\n",
    "    - ignoreFirstN: boolean determining whether to try to predict test scores where there are not 'nPreviousTimesteps' number of previous tests present in the same wake period to use as features. \n",
    "4. demographic_features: list of demographic features to use (options are given in SubjectInformation.csv)\n",
    "5. output_info: dictionary of information about outcome metric  \n",
    "    - outputDataType: type of data to use for outcome metric (PVT, DSST, ADD, or Moods)\n",
    "    - outputFeature: feature in the 'outputDataType' file to use as the outcome metric\n",
    "    - log_output: boolean whether to log the outcome metric\n",
    "6. pre_processing_info: dictionary that specifies pre-processing information\n",
    "    - normalize: boolean determining whether to normalize data\n",
    "    - imputationType: type of column-wise imputation. Options are 'drop' (drop missing data), or impute with 'mean', 'median', or 'mode'\n",
    "    - colsToNotNormalize: list of columns that should not be normalized\n",
    "\n",
    "### Data Collection\n",
    "Once the class is instantiated, the data is collected using the getData() function of the SlidingWindowDataCollection class. This function returns the collected dataframe as well as saving it to the Datasets folder. Note that this function, if there is already a file in the Datasets folder that used the same feature, will prompt the user about whether to overwrite the file or not. If 'yes', then re-collect data and overwrite existing file. If 'no', then use the pre-existing file as our data. It also writes a txt file containing the features used, and other parameter information for future reference.\n",
    "\n",
    "\n",
    "### Nested CV Splits\n",
    "The nested cross-validations splits are made by the split_data() function of the SlidingWindowDataCollection class. The function takes the following arguments:  \n",
    "1. collectedData: pandas dataframe that was collected using the getData() function\n",
    "2. split_info: dictionary of information about how to split data\n",
    "    - splitType: which nested cross-validation technique to use. Options are none, predictSecondHalf, populationInformedPredictSecondHalf, or populationInformedWPForwardChaining and are specified as strings\n",
    "    - studyChoice: (optional) list of study names to use in the dataset- all others are excluded\n",
    "    - peopleChoice: (optional) list of participant codes to use in the dataset- all others are excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowDataCollection():\n",
    "    \"\"\"This is a base class for all algorithms that use a sliding window\n",
    "    i.e. use n previous time steps to predict at the next time step\"\"\"\n",
    "        \n",
    "    def __init__(self, participants, sleep_info, test_info, demographic_features, output_info, pre_processing_info):\n",
    "        \"\"\" Initialize Sliding Window Data Collection\n",
    "        Parameters:\n",
    "        ----------\n",
    "        *participants: Participants object's participant_list attribute\n",
    "        *sleep_info: dictionary {nPreviousNights: #, sleepFeatures: list}\n",
    "        *test_info: dictionary {previousTestFeatures: list of lists [testName,testFeature], timingInfoPreviousTests: list, nPreviousTimesteps: #, timingInfoCurrentTest: list, ignoreFirstN: True/False}\n",
    "        *demographic_features: list of demographic features (ones in subject information file)\n",
    "        *output_info: dictionary {outputDataType: string, outputFeature: string, log_output: True/False}\n",
    "        *pre_processing_info: dictionary {imputationType: string, normalize: True/False, columnsToNotNormalize: list}\n",
    "        \"\"\"\n",
    "        \n",
    "        self.participants = participants\n",
    "        #sleep information\n",
    "        self.sleep_info = sleep_info\n",
    "        self.nPreviousNights = sleep_info['nPreviousNights']\n",
    "        self.sleepFeatures = sleep_info['sleepFeatures']\n",
    "        \n",
    "        #test information\n",
    "        self.test_info = test_info\n",
    "        self.previousTestFeatures = test_info['previousTestFeatures']\n",
    "        self.timingInfoPreviousTests = test_info['timingInfoPreviousTests']\n",
    "        self.nPreviousTimesteps = test_info['nPreviousTimesteps']\n",
    "        self.timingInfoCurrentTest = test_info['timingInfoCurrentTest']\n",
    "        self.ignoreFirstN = test_info['ignoreFirstN']\n",
    "        \n",
    "        #demographic features\n",
    "        self.demographicFeatures = demographic_features\n",
    "        \n",
    "        #output features\n",
    "        self.output_info = output_info\n",
    "        self.outputDataType = output_info['outputDataType']\n",
    "        self.outputFeature = output_info['outputFeature']\n",
    "        self.log_output = output_info['log_output']\n",
    "        self.output_variable = self.outputDataType+self.outputFeature+\"(t)\"\n",
    "        \n",
    "        #pre-processing\n",
    "        self.pre_processing_info = pre_processing_info\n",
    "        self.normalize = pre_processing_info['normalize']\n",
    "        self.columnsToNotNormalize = pre_processing_info['colsToNotNormalize']\n",
    "        self.imputationType = pre_processing_info['imputationType']\n",
    "        \n",
    "        self.data = []\n",
    "        self.column_names = []\n",
    "        self.current_row = []\n",
    "        self.current_names = []\n",
    "        self.collectedDataFrame = None\n",
    "\n",
    "    def getTimingFeature(self, feat, data_t, time, participant):\n",
    "        \"\"\"This function generates the timing features for the different tests\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        *feat: feature name, options are 'HoursAwake', 'NumWPonFD', and 'CircadianPhase',\n",
    "        NumWeekOnFD, SleepOpp72, HoursOnProtocol, SESSION, WP\n",
    "        *data_t: dataframe of data at the time of the test we are considering\n",
    "        *time: time of test we are considering\n",
    "        *participant: Participant object we are collecting data for\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        Value of the timing feature (float)\n",
    "        \n",
    "        \"\"\"\n",
    "        if len(data_t.index) == 0: #if we have no information for this time, return NaN\n",
    "            d = np.nan\n",
    "        elif feat == 'HoursAwake':\n",
    "            d = data_t['HoursAwake'].values[0]\n",
    "        elif feat == 'NumWPonFD':\n",
    "            d = data_t['WakePeriod'].values[0]-participant.startFDSPn\n",
    "        elif feat == 'NumWeekonFD':\n",
    "            time = data_t['DecimalTime'].values[0]\n",
    "            start_time = participant.startFDtime\n",
    "            if time <= start_time+24*7:\n",
    "                d = 1\n",
    "            elif time <= start_time+24*7*2:\n",
    "                d = 2\n",
    "            elif time <= start_time+24*7*3:\n",
    "                d = 3\n",
    "            elif time <= start_time+24*7*4:\n",
    "                d = 4\n",
    "            else:\n",
    "                assert 1==0, \"issue with numWeek\"\n",
    "        elif feat == 'SleepOpp72':\n",
    "            time_range = [data_t['DecimalTime'].values[0]-24*3,data_t['DecimalTime'].values[0]]\n",
    "            timings = pd.read_csv(\"SleepTimingFile.csv\")\n",
    "            participant = participant.participantCode\n",
    "            a = timings[timings.SUBJECT==participant]\n",
    "\n",
    "            sleep_time = 0\n",
    "            found = False\n",
    "            for i in zip(a['Start'].values, a['End'].values):\n",
    "                if time_range[0] >= i[0] and time_range[0] <= i[1]:\n",
    "                    sleep_time += i[1]-time_range[0]\n",
    "                    found = True\n",
    "                    break\n",
    "            if found:\n",
    "                end_val_SP = i[1]\n",
    "                remaining_time_range = [end_val_SP,time_range[1]]\n",
    "            else:\n",
    "                remaining_time_range = time_range\n",
    "                                  \n",
    "            for j in zip(a['Start'].values, a['End'].values):\n",
    "                if j[0] >= remaining_time_range[0] and j[1] <= remaining_time_range[1]:\n",
    "                    sleep_time += j[1]-j[0]\n",
    "            return sleep_time\n",
    "        elif feat == 'HoursOnProtocol':\n",
    "            d = data_t['DecimalTime'].values[0]-participant.startFDtime\n",
    "        elif feat == 'CircadianPhase':\n",
    "            d = data_t['CircadianPhase'].values[0]\n",
    "        elif feat == 'SESSION':\n",
    "            d = data_t['SESSION'].values[0]\n",
    "        elif feat == \"WP\":\n",
    "            d = data_t['WakePeriod'].values[0]\n",
    "        return d\n",
    "    \n",
    "    def getSleepData(self, participant, SPtoStart):\n",
    "        \"\"\"This function gets sleep metrics for a participant\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        *participant: Participant object we are collecting data for\n",
    "        *SPtoStart: number sleep period to start analysis at\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        #get SPn numbers to consider, starting at SPtoStart\n",
    "        sps_to_consider = [SPtoStart-i for i in range(self.nPreviousNights)] \n",
    "        d = participant.sleep\n",
    "\n",
    "        for sp in sps_to_consider:\n",
    "            for feat in self.sleepFeatures:\n",
    "                d2 = d[d.SPn == sp][feat].values\n",
    "                if len(d2) == 0:\n",
    "                    self.current_row += [np.nan]\n",
    "                elif len(d2) == 1:\n",
    "                    self.current_row += [d2[0]]\n",
    "                else:\n",
    "                    assert 1==0, 'More than 1 row data for SP'\n",
    "                self.current_names += [\"PreviousSleepPeriod\"+str(sps_to_consider.index(sp)+1)+feat]\n",
    "\n",
    "    def getPreviousTestData(self, participant, t, hourStep):\n",
    "        \"\"\"Gets data for the previous n tests and test timing information\n",
    "        The kth previous test (compared to the test at time t) is found by \n",
    "        looking at time t-k*hourStep. If the nearest test to that value is \n",
    "        within 30 minutes of it, then we use this test information. Otherwise\n",
    "        we consider this test missing.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        *participant: Participant object we are collecting data for\n",
    "        *t: time of the test we are trying to predict\n",
    "        *hourStep: the time interval between the test batteries\n",
    "        \"\"\"\n",
    "        names_to_data = {'ADD':participant.add, 'DSST':participant.dsst, 'PVT':participant.pvt, 'Moods':participant.moods}\n",
    "        \n",
    "        for testInfo in self.previousTestFeatures:\n",
    "            testName = testInfo[0]\n",
    "            testFeature = testInfo[1]\n",
    "            data = names_to_data[testName]\n",
    "            \n",
    "            previousData = data[data.DecimalTime < t] #get all previous data\n",
    "\n",
    "            for n in range(self.nPreviousTimesteps-1,-1,-1):\n",
    "                c = previousData.iloc[(previousData['DecimalTime']-(t-hourStep*(n+1))).abs().argsort()[0:1]]\n",
    "\n",
    "                if len(c.index) == 1:\n",
    "                    closest_time =  c.DecimalTime.values[0]\n",
    "                    if abs(closest_time-(t-hourStep*(n+1))) <= 0.5:\n",
    "                        closest_time =  c.DecimalTime.values[0]\n",
    "                        closest_score = (previousData[previousData.DecimalTime == closest_time][testFeature].values[0])\n",
    "                    else:\n",
    "                        closest_time = np.nan\n",
    "                        closest_score = np.nan\n",
    "                    time_diff = t-closest_time\n",
    "\n",
    "                elif len(c.index) == 0:\n",
    "                    closest_time = np.nan\n",
    "                    closest_score = np.nan\n",
    "                    time_diff = np.nan\n",
    "                else:\n",
    "                    assert 1==0,'More than one test at given time'+ str(t)\n",
    "\n",
    "                self.current_row += [closest_score, closest_time, time_diff] \n",
    "                self.current_names += [str(testName)+str(testFeature)+\"(t-\"+str(n+1)+\")\",str(testName)+str(testFeature)+\"(t-\"+str(n+1)+\")Time\",str(testName)+str(testFeature)+\"(t-\"+str(n+1)+\")TimeDiff\"]\n",
    "\n",
    "                #get test timing information\n",
    "                for i in self.timingInfoPreviousTests:\n",
    "                    self.current_row += [self.getTimingFeature(i, previousData[previousData.DecimalTime == closest_time], closest_time, participant)] \n",
    "                    self.current_names += [str(testName)+str(testFeature)+\"(t-\"+str(n+1)+\")\"+i]\n",
    "\n",
    "    def collect_data(self):\n",
    "        \"\"\"This method constructs the data set for sliding window algorithms (only considers FD)\n",
    "        \n",
    "        Note: n previous tests aren't necessarily in the same WP (we only make sure that the output test\n",
    "        has n previous tests in the same wake period if attribute ignoreFirstN = True in main driver function)\n",
    "        \"\"\"\n",
    "        for participant in self.participants:\n",
    "            names_to_data = {'ADD':participant.add, 'DSST':participant.dsst, 'PVT':participant.pvt, 'Moods':participant.moods}\n",
    "\n",
    "            LabelData = names_to_data[self.outputDataType]\n",
    "            LabelDataFD = LabelData[(LabelData.DecimalTime >= participant.startFDtime) & (LabelData.DecimalTime <= participant.endFDtime)]\n",
    "            times = sorted(LabelDataFD.DecimalTime.values)\n",
    "            dataset = []\n",
    "\n",
    "            for t in range(len(times)):\n",
    "                self.current_row = [times[t]]\n",
    "                self.current_names = ['DecimalTime']\n",
    "                data_t = LabelDataFD[LabelDataFD.DecimalTime == times[t]]\n",
    "                label_t = data_t[self.outputFeature].values[0]\n",
    "                wp = data_t['WakePeriod'].values[0]\n",
    "                study = participant.study\n",
    "\n",
    "                if self.ignoreFirstN: #check if the previous n output values are within same WP\n",
    "                    data_prev = LabelDataFD[LabelDataFD.DecimalTime.isin([times[t-(i+1)] for i in range(self.nPreviousTimesteps)])]\n",
    "                    valid_output = (len(set(data_prev['WakePeriod'].values)) == 1) & (data_t['WakePeriod'].values[0] == data_prev['WakePeriod'].values[0])\n",
    "                else:\n",
    "                    valid_output = True\n",
    "\n",
    "                if valid_output and not np.isnan(label_t): #valid output, and make sure label is not missing\n",
    "                    #get data for previous test features and test timing\n",
    "                    if participant.study == 'AFOSR9':\n",
    "                        self.getPreviousTestData(participant, times[t],4)\n",
    "                    else:\n",
    "                        self.getPreviousTestData(participant, times[t],2)\n",
    "\n",
    "                    #get sleep data\n",
    "                    self.getSleepData(participant, wp-1)\n",
    "\n",
    "                    #get testTiming information\n",
    "                    for i in self.timingInfoCurrentTest:\n",
    "                        self.current_row += [self.getTimingFeature(i, data_t, times[t], participant)]\n",
    "                        self.current_names += [i+'(t)']\n",
    "\n",
    "                    #get demographic information\n",
    "                    for i in self.demographicFeatures:\n",
    "                        self.current_row += [getattr(participant, i)]\n",
    "                        self.current_names += [i]\n",
    "\n",
    "                    #get label\n",
    "                    self.current_row += [label_t]\n",
    "                    self.current_names += [self.outputDataType+self.outputFeature+\"(t)\"]\n",
    "\n",
    "                    self.data.append(self.current_row)\n",
    "                    self.column_names = self.current_names\n",
    "\n",
    "        self.collectedDataFrame = pd.DataFrame(self.data,columns=self.column_names)\n",
    "        return self.collectedDataFrame\n",
    "    \n",
    "    def getData(self):\n",
    "        \"\"\"Function that collects sliding window data, writes to file, and returns a dataframe\n",
    "        If the data already exists, prompts user whether to overwrite.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        Returns dataframe and also writes data to file in Datasets folder\n",
    "        \"\"\"\n",
    "        collect_data = False    \n",
    "            \n",
    "        fname = str(self.nPreviousTimesteps)+\"previousSteps,\"+'&'.join([str(i[1]) for i in self.previousTestFeatures]) + str(self.nPreviousNights)+\"night\"+str(len(self.sleepFeatures))+\"->\"+self.outputFeature\n",
    "        if os.path.exists('Datasets/'+fname+'.csv'):\n",
    "            response = input('Recompile this file '+fname+\"?\")\n",
    "            if response == 'no':\n",
    "                print(\"Using previous saved Data file\",fname)\n",
    "                collectedData = pd.read_csv('Datasets/'+fname+'.csv')\n",
    "            elif response == 'yes':  #need to delete file\n",
    "                collect_data = True\n",
    "            else:\n",
    "                print(\"Issue\")\n",
    "                collect_data = True\n",
    "        else:\n",
    "            print('Didnt find file, collecting new data')\n",
    "            collect_data = True\n",
    "            \n",
    "        if collect_data:\n",
    "            collectedData = self.collect_data()\n",
    "            num_features = len(list(collectedData))-1\n",
    "            \n",
    "            collectedData.to_csv(\"Datasets/\"+fname+'.csv', index=False)\n",
    "            print(\"Collected Data for Filename\"+fname)\n",
    "\n",
    "            #write data information to txt file\n",
    "            with open(\"Datasets/\"+fname+'.txt', \"w\") as text_file:\n",
    "                text_file.write(\"Sleep Info: \"+str(self.sleep_info)+\"\\n\") \n",
    "                text_file.write(\"Test Info: \"+str(self.test_info)+\"\\n\") \n",
    "                text_file.write(\"Demographic Info: \"+str(self.demographicFeatures)+\"\\n\") \n",
    "                text_file.write(\"Output Info: \"+str(self.output_info)+\"\\n\") \n",
    "                text_file.write(\"Pre-processing Info: \"+str(self.pre_processing_info)+\"\\n\") \n",
    "        return collectedData\n",
    "    \n",
    "    def noSplits(self, df):\n",
    "        \"\"\" Returns all the data as both training, validation, and\n",
    "        testing sets\"\"\"\n",
    "        return [df], [df], [df]\n",
    "    \n",
    "    def predictSecondHalf(self, df):\n",
    "        \"\"\" This implements Predict Second Half nested cross-validation \n",
    "        The original training and test split is done by splitting the wake\n",
    "        periods in half. Then the validation set is the last three wake \n",
    "        periods of the training set.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        *df: dataframe of all data \n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        *training_sets: list of dataframes to use for training data\n",
    "        *testing_sets: list of dataframes to use for testing data\n",
    "        *validation_sets: list of dataframes to use for validation data\n",
    "        \n",
    "        Ex. \n",
    "        Patient 1: [A1,A2,A3,A4,A5,A6,A7,A8,A9,A10]\n",
    "        Patient 2: [B1,B2,B3,B4,B5,B6,B7,B8,B9,B10,B11,B12]\n",
    "        \n",
    "        Train/Test: train = [A1,B1-B2], validate = [A2-A4,B3-B5], test = [A5-A10,B6-B12]\n",
    "        \"\"\"\n",
    "        training_data = []\n",
    "        testing_data = []\n",
    "        validation_data = []\n",
    "        \n",
    "        for participant in list(set(df['participantCode'])):\n",
    "            df2 = df[df.participantCode == participant]\n",
    "            df2 = df2.sort_values(by=['DecimalTime']) #make sure the data is ordered\n",
    "            \n",
    "            wakePeriods = sorted(list(set(df2['WP(t)'])))\n",
    "            train_up_to = wakePeriods[int(len(wakePeriods)/2.0)]\n",
    "        \n",
    "            train = df[((df.participantCode == participant) & (df['WP(t)'] <= train_up_to-3))].reset_index(drop=True)\n",
    "            validation = df[((df.participantCode == participant) & (df['WP(t)'] >= train_up_to-2)&(df['WP(t)'] <= train_up_to))].reset_index(drop=True)\n",
    "            test = df[(df.participantCode == participant) & (df['WP(t)'] > train_up_to)].reset_index(drop=True)\n",
    "\n",
    "            training_data.append(train)\n",
    "            testing_data.append(test)\n",
    "            validation_data.append(validation)\n",
    "            \n",
    "        try:\n",
    "            training_sets = pd.concat(training_data).reset_index(drop=True)\n",
    "            validation_sets = pd.concat(validation_data).reset_index(drop=True)\n",
    "            testing_sets = pd.concat(testing_data).reset_index(drop=True)\n",
    "            return [training_sets], [testing_sets], [validation_sets]\n",
    "        except:\n",
    "            return [training_data[0]], [testing_data[0]], [validation_data[0]]\n",
    "    \n",
    "    def populationInformedPredictSecondHalf(self,df):\n",
    "        \"\"\" This implements population-informed Predict Second Half nested cross-validation \n",
    "        The code currently uses the last wake period in the training set\n",
    "        as the validation set.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        *df: dataframe of all data \n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        *training_sets: list of dataframes to use for training data\n",
    "        *testing_sets: list of dataframes to use for testing data\n",
    "        *validation_sets: list of dataframes to use for validation data\n",
    "        \n",
    "        Ex. \n",
    "        Patient 1: [A1,A2,A3,A4,A5,A6]\n",
    "        Patient 2: [B1,B2,B3,B4,B5,B6]\n",
    "        \n",
    "        Train/Test #1: train = [A1-A2,B1-B6], validate = [A3], test = [A4-A6]\n",
    "        Train/Test #2: train = [A1-A6,B1-B2], validate = [B3], test = [B4-B6]\n",
    "        \"\"\"\n",
    "        \n",
    "        training_data = []\n",
    "        testing_data = []\n",
    "        validation_data = []\n",
    "        \n",
    "        for participant in list(set(df['participantCode'])):\n",
    "            df2 = df[df.participantCode == participant]\n",
    "            df2 = df2.sort_values(by=['DecimalTime']) #make sure the data is ordered\n",
    "            \n",
    "            wakePeriods = sorted(list(set(df2['WP(t)'])))\n",
    "            train_up_to = wakePeriods[int(len(wakePeriods)/2.0)]\n",
    "        \n",
    "            train = df[((df.participantCode == participant) & (df['WP(t)'] <= train_up_to-1)) | (df.participantCode != participant)].reset_index(drop=True)\n",
    "            validation = df[((df.participantCode == participant) & (df['WP(t)'] == train_up_to))].reset_index(drop=True)\n",
    "            test = df[(df.participantCode == participant) & (df['WP(t)'] > train_up_to)].reset_index(drop=True)\n",
    "\n",
    "            training_data.append(train)\n",
    "            testing_data.append(test)\n",
    "            validation_data.append(validation)\n",
    "        return training_data, testing_data, validation_data\n",
    "    \n",
    "    def populationInformedWPForwardChaining(self, df):\n",
    "        \"\"\" This implements population-informed Wake Period\n",
    "        Forward Chaining nested cross-validation. The code \n",
    "        currently uses the last wake period in the training set\n",
    "        as the validation set.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        *df: dataframe of all data \n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        *training_sets: list of dataframes to use for training data\n",
    "        *testing_sets: list of dataframes to use for testing data\n",
    "        *validation_sets: list of dataframes to use for validation data\n",
    "        \n",
    "        Ex. \n",
    "        Patient 1: [A1,A2,A3,A4,A5,A6]\n",
    "        Patient 2: [B1,B2,B3,B4,B5,B6]\n",
    "        \n",
    "        Train/Test #1: train = [A1,B1-B5], validate = [A2], test = [A3]\n",
    "        Train/Test #2: train = [A1,A2,B1-B5], validate = [A3], test = [A4]\n",
    "        Train/Test #3: train = [A1,A2,A3,B1-B5], validate = [A4], test = [A5]\n",
    "        Train/Test #4: train = [A1,A2,A3,A4,B1-B5], validate = [A5], test = [A6]\n",
    "        Train/Test #5: train = [A1-A5,B1], validate = [B2], test = [B3]\n",
    "        Train/Test #6: train = [A1-A5,B1,B2],validate = [B3], test = [B4]\n",
    "        Train/Test #7: train = [A1-A5,B1,B2,B3], validate = [B4], test = [B5]\n",
    "        Train/Test #8: train = [A1-A5,B1,B2,B3,B4], validate = [B5], test = [B6]\n",
    "        \"\"\"\n",
    "        \n",
    "        training_sets = []\n",
    "        testing_sets = []\n",
    "        validation_sets = []\n",
    "        colnames = list(df)\n",
    "        \n",
    "        for participant in list(set(df['participantCode'])):\n",
    "            df2 = df[df.participantCode == participant]\n",
    "            df2 = df2.sort_values(by=['DecimalTime']) #make sure the data is ordered\n",
    "            \n",
    "            wakePeriods = sorted(list(set(df2['WP(t)'])))\n",
    "            \n",
    "            for wp in range(2,len(wakePeriods)):\n",
    "                train = df[((df.participantCode == participant) & (df['WP(t)'] <= wakePeriods[wp-2])) | (df.participantCode != participant)].reset_index(drop=True)\n",
    "                validation = df[((df.participantCode == participant) & (df['WP(t)'] == wakePeriods[wp-1]))].reset_index(drop=True)\n",
    "                test = df[(df.participantCode == participant) & (df['WP(t)'] == wakePeriods[wp])].reset_index(drop=True)\n",
    "            \n",
    "                training_sets.append(train)\n",
    "                testing_sets.append(test)\n",
    "                validation_sets.append(validation)\n",
    "        return training_sets, testing_sets, validation_sets\n",
    "    \n",
    "    def imputeTypes(self, values):\n",
    "        \"\"\"Calculates the value to impute with\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        *values: values to impute\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Returns value to impute by\n",
    "        \"\"\"\n",
    "        if self.imputationType == 'mean':\n",
    "            return values.mean(axis=0)\n",
    "        elif self.imputationType == 'median':\n",
    "            return values.median(axis=0)\n",
    "        elif self.imputationType == 'mode':\n",
    "            return values.mode(axis=0)\n",
    "        else:\n",
    "            assert 1==0,'Imputation Type Not Implemented'\n",
    "        \n",
    "    def imputeAndNormalize(self, train, test, validate):\n",
    "        \"\"\"Function that imputes and normalizes data for each\n",
    "        of the training/validation/testing splits. Imputation\n",
    "        and normalization is performed column-wise based on the \n",
    "        values calculated from the training set.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        *train: training data to impute/normalize\n",
    "        *test: test data to impute/normalize\n",
    "        *validate: test data to impute/normalize\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        Training and test data (now imputed and normalized as needed)\n",
    "        \"\"\"\n",
    "        all_col = list(train)\n",
    "        columnsToNormalize = [i for i in all_col if i not in self.columnsToNotNormalize]\n",
    "        \n",
    "        if self.imputationType == 'drop':\n",
    "            train = train.dropna(axis=0, how='any')\n",
    "            test = test.dropna(axis=0, how='any')\n",
    "            validate = validate.dropna(axis=0, how='any')\n",
    "        else:\n",
    "            fillwith = self.imputeTypes(train)\n",
    "            train = train.fillna(fillwith)\n",
    "            test = test.fillna(fillwith)\n",
    "            validate = validate.fillna(fillwith)\n",
    "        \n",
    "        for col in columnsToNormalize:\n",
    "            test.loc[:,\"Unnormalized:\"+col] = test[col]\n",
    "            train.loc[:,\"Unnormalized:\"+col] = train[col]\n",
    "            validate.loc[:,\"Unnormalized:\"+col] = validate[col]\n",
    "        \n",
    "        means = train[columnsToNormalize].mean()\n",
    "        stds = train[columnsToNormalize].std(ddof=0)    \n",
    "        train[columnsToNormalize] = (train[columnsToNormalize]-means)/stds\n",
    "        test[columnsToNormalize] = (test[columnsToNormalize]-means)/stds\n",
    "        validate[columnsToNormalize] = (validate[columnsToNormalize]-means)/stds\n",
    "\n",
    "        return train, test, validate\n",
    "    \n",
    "    def getSplitImputedNormalizedData(self, df, split_info):\n",
    "        \"\"\"Function that splits dataset, imputes, normalizes, and\n",
    "        converts output variable, as necessary. The cross-validation\n",
    "        splits are determined by the 'splitType' value of the split_info\n",
    "        dictionary which can take the value of: 'none', 'predictSecondHalf',\n",
    "        'populationInformedPredictSecondHalf', or 'populationInformedWPForwardChaining'\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        *df: dataframe containing collected data\n",
    "        *split_info: dictionary that contains preferences for splitting\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        Imputed/Normalized training sets and Imputed/Normalized test sets\n",
    "        \"\"\"\n",
    "        #split the datasets\n",
    "        if split_info['splitType'] == 'populationInformedWPForwardChaining':\n",
    "            training_sets, testing_sets, validation_sets = self.populationInformedWPForwardChaining(df)\n",
    "        elif split_info['splitType'] == 'none':\n",
    "            training_sets, testing_sets, validation_sets = self.noSplits(df)\n",
    "        elif split_info['splitType'] == 'predictSecondHalf':\n",
    "            training_sets, testing_sets, validation_sets = self.predictSecondHalf(df)\n",
    "        elif split_info['splitType'] == 'populationInformedPredictSecondHalf':\n",
    "            training_sets, testing_sets, validation_sets = self.populationInformedPredictSecondHalf(df)\n",
    "        else:\n",
    "            assert 1==0, \"Split Type not Implemented\"\n",
    "        imputedNormalized_training_sets = []\n",
    "        imputedNormalized_testing_sets = []\n",
    "        imputedNormalized_validation_sets = []\n",
    "        \n",
    "        print(\"Splits made.....\")\n",
    "        all_col = list(df)\n",
    "        columnsToNormalize = [i for i in all_col if i not in self.columnsToNotNormalize]\n",
    "        print(\"Imputing and Normalizing Columns\",columnsToNormalize)\n",
    "        #Iterate through the splits and impute and normalize\n",
    "        for i in range(len(training_sets)):\n",
    "            train = training_sets[i]\n",
    "            test = testing_sets[i]\n",
    "            validate = validation_sets[i]\n",
    "            \n",
    "            imputedNormalized_trainingSet, imputedNormalized_testingSet, imputedNormalized_validationSet = self.imputeAndNormalize(train, test, validate)\n",
    "            if self.log_output:\n",
    "                imputedNormalized_trainingSet[self.output_variable] = np.log(imputedNormalized_trainingSet[self.output_variable])\n",
    "                imputedNormalized_testingSet[self.output_variable] = np.log(imputedNormalized_testingSet[self.output_variable])\n",
    "                imputedNormalized_validationSet[self.output_variable] = np.log(imputedNormalized_validationSet[self.output_variable])\n",
    "\n",
    "                imputedNormalized_trainingSet.rename(columns={self.output_variable: 'Log:'+self.output_variable}, inplace=True)\n",
    "                imputedNormalized_testingSet.rename(columns={self.output_variable: 'Log:'+self.output_variable}, inplace=True)\n",
    "                imputedNormalized_validationSet.rename(columns={self.output_variable: 'Log:'+self.output_variable}, inplace=True)\n",
    "\n",
    "            imputedNormalized_training_sets.append(imputedNormalized_trainingSet)\n",
    "            imputedNormalized_testing_sets.append(imputedNormalized_testingSet)\n",
    "            imputedNormalized_validation_sets.append(imputedNormalized_validationSet)\n",
    "        \n",
    "        return imputedNormalized_training_sets, imputedNormalized_testing_sets, imputedNormalized_validation_sets\n",
    "    \n",
    "    def split_data(self, collectedData, split_info):\n",
    "        \"\"\" Function that splits the data into training, validation, and\n",
    "        testing sets based on the 'splitType' variable in the split_info \n",
    "        dictionary. If split_info contains a 'studyChoice' variable, then\n",
    "        only include participants from given study. If split_info contains\n",
    "        a 'peopleChoice' variable, then only include specified individuals. \n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        *collectedData: dataframe of collected data\n",
    "        *split_info: dictionary of splitting information\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        *training_sets: list of dataframes of training data, all imputed and normalized\n",
    "        *validation_sets: list of dataframes of validation data, all imputed and normalized\n",
    "        *testing_sets: list of dataframes of testing data, all imputed and normalized\n",
    "        \"\"\"\n",
    "        \n",
    "        if 'studyChoice' in split_info:\n",
    "            collected_data_new = collectedData[collectedData.study.isin(split_info['studyChoice'])]\n",
    "        elif 'peopleChoice' in split_info:\n",
    "            collected_data_new = collectedData[collectedData.participantCode.isin(split_info['peopleChoice'])]   \n",
    "        else:\n",
    "            collected_data_new = collectedData\n",
    "\n",
    "        training_sets, testing_sets, validation_sets = self.getSplitImputedNormalizedData(collected_data_new, split_info)\n",
    "        return training_sets, testing_sets, validation_sets \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
